---
title: Can AI think? (part 2)
version: v1
author: Rev. Thomas J. Pulickal
tags:
- ai
---
## Embodying intelligence and being intelligent
If on our first manned trip to Mars, we discovered an orrery (mechanical model of the solar system that predicts the positions of celestial bodies), we would be certain that some intelligent being had been there. This is because the orrery is a small object that condenses immense complexity in such a way that is only possible if there is an understanding about the patterns of movement and their interrelatedness. Indeed, two simultaneous signs of the intelligence of a machine or an idea are their explanatory and predictive power. [Watching an orrery](https://www.youtube.com/watch?v=yKS7C0dC-bU), you can see both in clear display. 

What is the precise relationship between intelligence and the orrery? Because it patterns certain underlying structures of the universe and both explains what we see in the sky and predicts what we will see in the future, the machine embodies intelligence. In some respect, it does not matter who or what made it. From the evolutionary perspective, this embodiment of intelligence is ubiquitous on earth and seemingly absent as far and wide as we have looked in the universe. Every creature that exists on earth today is able to live because of how well it embodies the underlying patterns of the environment, whether in its neural network or in its physiology. These underlying patterns include earth's gravitational force, the composition of air, the behaviors of other organisms, the density of water (notice how well-adapted the densities of animals are to water), etc. In a very real sense, the orrery is to the solar system what the whale is to the ocean. The question with regards to both is not whether they embody intelligence but whether they *are* intelligent.

To begin with, we notice that the orrery does not *know* that it embodies intelligence, even though it can predict the movements of the moon better than almost everyone on earth. I believe that the same could be said about the whale. And the same could be said about artificial intelligence. Of course, this only pushes the question further back. What does it mean to *know*? We want to avoid the tangle of unresolvable questions, especially ones that have an inherent circuitousness to them (i.e. knowing the meaning of knowing). Instead, let us take note of certain clues that tell us artificial intelligence is something fundamentally different from being intelligent.

## Why LLMs need to run code
If you ask a Large Language Model an uncommon question that involves several steps, it will execute code before it responds to you. For example, I asked two LLMs a question. One just happened to use code and the other did not.

**[ChatGPT](https://chatgpt.com/share/6912631b-d1c4-8012-8703-4b4bc50ec545)**
![ChatGPT not using code]({{ side.media }}./chatgpt_nocode.png)

**[Grok 4 Fast](https://grok.com/share/bGVnYWN5_85b255c1-1507-456d-85c2-49d9f55933fc)**
![Grok 4 Fast using code]({{ site.media }}./grok4_code.png)

As you can see, Grok arrived at the correct answer while ChatGPT did not (though it of course employs a tone of finality). Grok wrote code that it then executed to help it find the solutions, which you can see by clicking into the link above and expanding the section that says, "Thought for 1m 1s." This is an important clue about the limitations of AI, at least as we know it today. A controversial paper published this summer, "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity,"[^1] from a research team at Apple makes a similar observation and gives a detailed explanation of when and why code is needed. It was controversial because some believed that executing code is "fair game" for an AI model, while others believe that this demonstrates that the AI is not truly thinking.

## 

[^1]: Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, and Mehrdad Farajtabar, ["The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity,"](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf) Apple Machine Learning Research, June 7, 2025.